Create & Manage RAID 6
--------------------------------------------------------

Striping with double parity

Pre requisites :-

 Minimum  ---> 4 disks
 Maximum  ---> 16 disks




                                         RAID 6
                                [Striping + Double Parity across drives]
                                           |
					   |
                    ________________________________________________________________
		   |		   |     				|	    |
               [block 1a]       [block 1b]                     [block b1]**   [block b1]**
               [block 2a]       [block 2b]**                   [block b2]**   [block 2b]
               [block b3]**     [block b3]**                   [block 3a]     [block 3b]
               [block b4]**     [block 4a]                     [block 4b]     [block b4]**
                drive1            drive2                        drive3          drive4 



data ----> 2 disks store
checksum of data ---> 3rd disk
 copy of 3rd disk ---> 4th disk


Advantages :-

> Read fast 
> if two disk fail. data  have access. we can replace

[ RAID 6 >>>>> RAID 5 in terms of secure ]


Dis Advantages:-

> write data  slow [20% slow than RAID 5] ---> not preferred for DB server [CRUD operations]
> drive fail effect on throughput
> Complex ---> Larger disk failed ---> data rebuilding /recovering take large time



Uses :-

> RAID 6 ---> performance + safety data [ better than RAID 5 ]
> for application servers preferable


++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++


                                  N = 1011  0101
                                           |
					   |
                    _____________________________________________________________
		   |	          	  |     		|	         |
               [1011] <--chksum parity--> [0101]               [    ]	       [    ]
               [    ]    Data Bits        [    ]               [0110]          [0110]        ---> [1011 - 0101= 0110]  Parity Bits
               [    ]                     [    ]               [    ]          [    ]
               [    ]                     [    ]               [    ]          [    ]
               [    ]                     [    ]               [    ]          [    ]
               [    ]                     [    ]               [    ]          [    ]
              Disk 1 [1 TB]             Disk 2[1TB]          Disk 3[1TB]       Disk
                     RAID capacity=2TB
                                   

  					  RAID 5
                                [Striping + Parity across drives]

 Data stored Randomly not in particular block. system will manage 

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

RAID 6

=============================================================================================


1. we need to create RAID 6  for sda-d


[root@localhost ~]# lsblk
NAME          MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
sda             8:0    0    4G  0 disk 
sdb             8:16   0    4G  0 disk 
sdc             8:32   0    4G  0 disk 
sdd             8:48   0    4G  0 disk 
sde             8:64   0    4G  0 disk 
sdf             8:80   0    4G  0 disk 
sr0            11:0    1 1024M  0 rom  
nvme0n1       259:0    0   30G  0 disk 
├─nvme0n1p1   259:1    0    1G  0 part /boot
└─nvme0n1p2   259:2    0   29G  0 part 
  ├─rhel-root 253:0    0   26G  0 lvm  /
  └─rhel-swap 253:1    0    3G  0 lvm  [SWAP]
[root@localhost ~]# mdadm  --create /dev/md0 -l 6 -n 4  /dev/sda /dev/sdb /dev/sdc /dev/sdd 
mdadm: Defaulting to version 1.2 metadata
mdadm: array /dev/md0 started.
[root@localhost ~]# 


************************************************************************************

2. Checking RAID 6 disk 


[root@localhost ~]# cat /proc/mdstat 
Personalities : [raid6] [raid5] [raid4] 
md0 : active raid6 sdd[3] sdc[2] sdb[1] sda[0]
      8378368 blocks super 1.2 level 6, 512k chunk, algorithm 2 [4/4] [UUUU]
      
unused devices: <none>
[root@localhost ~]

**************************************************************************************

3. Format & Mounting the FS

[root@localhost ~]# mkfs.ext4  /dev/md0 
mke2fs 1.45.6 (20-Mar-2020)
Creating filesystem with 2094592 4k blocks and 524288 inodes
Filesystem UUID: 02f317aa-4d8c-4dad-8439-7e92a4be799b
Superblock backups stored on blocks: 
	32768, 98304, 163840, 229376, 294912, 819200, 884736, 1605632

Allocating group tables: done                            
Writing inode tables: done                            
Creating journal (16384 blocks): done
Writing superblocks and filesystem accounting information: done 

[root@localhost ~]# lsblk -f
NAME          FSTYPE            LABEL                   UUID                                   MOUNTPOINT
sda           linux_raid_member localhost.localdomain:0 b0712a85-912d-6979-07ed-5a57722450e3   
└─md0         ext4                                      02f317aa-4d8c-4dad-8439-7e92a4be799b   
sdb           linux_raid_member localhost.localdomain:0 b0712a85-912d-6979-07ed-5a57722450e3   
└─md0         ext4                                      02f317aa-4d8c-4dad-8439-7e92a4be799b   
sdc           linux_raid_member localhost.localdomain:0 b0712a85-912d-6979-07ed-5a57722450e3   
└─md0         ext4                                      02f317aa-4d8c-4dad-8439-7e92a4be799b   
sdd           linux_raid_member localhost.localdomain:0 b0712a85-912d-6979-07ed-5a57722450e3   
└─md0         ext4                                      02f317aa-4d8c-4dad-8439-7e92a4be799b   



[root@localhost ~]# mkdir /raid6
[root@localhost ~]# mount /dev/md0  /raid6/
[root@localhost ~]# df -hT /raid6/
Filesystem     Type  Size  Used Avail Use% Mounted on
/dev/md0       ext4  7.8G   24K  7.4G   1% /raid6
[root@localhost ~]# 
************************************************************************************************

4. Storing data 


[root@localhost ~]# cd /raid6/
[root@localhost raid6]# touch {1..10}
[root@localhost raid6]# cal > cal.txt
[root@localhost raid6]# echo "Linux classes" > linux.txt
[root@localhost raid6]# mkdir aa bb cc dd
[root@localhost raid6]# ls
1  10  2  3  4  5  6  7  8  9  aa  bb  cal.txt  cc  dd  linux.txt  lost+found
[root@localhost raid6]# 

*******************************************************************************************************


5. failing /dev/sda one disk 


[root@localhost raid6]# cat /proc/mdstat 
Personalities : [raid6] [raid5] [raid4] 
md0 : active raid6 sdd[3] sdc[2] sdb[1] sda[0]
      8378368 blocks super 1.2 level 6, 512k chunk, algorithm 2 [4/4] [UUUU]
      
unused devices: <none>
[root@localhost raid6]# mdadm  /dev/md0  -f /dev/sda 
mdadm: set /dev/sda faulty in /dev/md0
[root@localhost raid6]# mdadm -D  /dev/md0  | grep -Ei 'State|Active|Failed|Working|Faulty'
             State : clean, degraded 
    Active Devices : 3
   Working Devices : 3
    Failed Devices : 1
    Number   Major   Minor   RaidDevice State
       1       8       16        1      active sync   /dev/sdb
       2       8       32        2      active sync   /dev/sdc
       3       8       48        3      active sync   /dev/sdd
       0       8        0        -      faulty   /dev/sda
[root@localhost raid6]# 


No issues with data

[root@localhost raid6]# cat linux.txt 
Linux classes
[root@localhost raid6]# ll
total 40
-rw-r--r--. 1 root root     0 May 11 16:14 1
-rw-r--r--. 1 root root     0 May 11 16:14 10
-rw-r--r--. 1 root root     0 May 11 16:14 2
-rw-r--r--. 1 root root     0 May 11 16:14 3
-rw-r--r--. 1 root root     0 May 11 16:14 4
-rw-r--r--. 1 root root     0 May 11 16:14 5
-rw-r--r--. 1 root root     0 May 11 16:14 6
-rw-r--r--. 1 root root     0 May 11 16:14 7
-rw-r--r--. 1 root root     0 May 11 16:14 8
-rw-r--r--. 1 root root     0 May 11 16:14 9
drwxr-xr-x. 2 root root  4096 May 11 16:15 aa
drwxr-xr-x. 2 root root  4096 May 11 16:15 bb
-rw-r--r--. 1 root root   168 May 11 16:14 cal.txt
drwxr-xr-x. 2 root root  4096 May 11 16:15 cc
drwxr-xr-x. 2 root root  4096 May 11 16:15 dd
-rw-r--r--. 1 root root    14 May 11 16:14 linux.txt
drwx------. 2 root root 16384 May 11 16:12 lost+found
[root@localhost raid6]# 


****************************************************************************************************

6.Failing another disk /dev/sdb


[root@localhost raid6]# mdadm  /dev/md0  -f /dev/sdb
mdadm: set /dev/sdb faulty in /dev/md0


[root@localhost raid6]# cat /proc/mdstat 
Personalities : [raid6] [raid5] [raid4] 
md0 : active raid6 sdd[3] sdc[2] sdb[1](F) sda[0](F)
      8378368 blocks super 1.2 level 6, 512k chunk, algorithm 2 [4/2] [__UU]
      
unused devices: <none>
[root@localhost raid6]# 




[root@localhost raid6]# mdadm -D  /dev/md0  | grep -Ei 'State|Active|Failed|Working|Faulty'
             State : clean, degraded 
    Active Devices : 2
   Working Devices : 2
    Failed Devices : 2
    Number   Major   Minor   RaidDevice State
       2       8       32        2      active sync   /dev/sdc
       3       8       48        3      active sync   /dev/sdd
       0       8        0        -      faulty   /dev/sda
       1       8       16        -      faulty   /dev/sdb
[root@localhost raid6]# 

***************************************************************************************************************


7. Removing the failed/faulty disks


[root@localhost raid6]# mdadm /dev/md0  -r  /dev/sda 
mdadm: hot removed /dev/sda from /dev/md0
[root@localhost raid6]# mdadm /dev/md0 -r /dev/sdb 
mdadm: hot removed /dev/sdb from /dev/md0
[root@localhost raid6]# 


[root@localhost raid6]# mdadm -D  /dev/md0  | grep -Ei 'State|Active|Failed|Working|Faulty|Removed'
             State : clean, degraded 
    Active Devices : 2
   Working Devices : 2
    Failed Devices : 0
    Number   Major   Minor   RaidDevice State
       -       0        0        0      removed
       -       0        0        1      removed
       2       8       32        2      active sync   /dev/sdc
       3       8       48        3      active sync   /dev/sdd
[root@localhost raid6]# 
********************************************************************************************************

8.Adding two New disks of same size 


[root@localhost raid6]# mdadm /dev/md0 -a /dev/sde
mdadm: added /dev/sde
[root@localhost raid6]# mdadm /dev/md0 -a /dev/sdf
mdadm: added /dev/sdf

Rebuilding /Recoverying the data

[root@localhost raid6]# cat /proc/mdstat 
Personalities : [raid6] [raid5] [raid4] 
md0 : active raid6 sdf[5] sde[4] sdd[3] sdc[2]
      8378368 blocks super 1.2 level 6, 512k chunk, algorithm 2 [4/2] [__UU]
      [====>................]  recovery = 24.9% (1047312/4189184) finish=0.2min speed=209462K/sec
      
unused devices: <none>
[root@localhost raid6]#

***********************************************************************************************************************

9. data has no impact even though two disks are failed and replaced

[root@localhost raid6]# mdadm -D  /dev/md0  | grep -Ei 'State|Active|Failed|Working|Faulty|Removed'
             State : clean 
    Active Devices : 4
   Working Devices : 4
    Failed Devices : 0
    Number   Major   Minor   RaidDevice State
       4       8       64        0      active sync   /dev/sde
       5       8       80        1      active sync   /dev/sdf
       2       8       32        2      active sync   /dev/sdc
       3       8       48        3      active sync   /dev/sdd
[root@localhost raid6]# 

****************************************************************************************************************************


Comments:-

1. Physical RAID needs to be careful while removing disk
2. HBA status ---> rhel4
3. KVM virtualization ---> Cockpit & libvert package  --> manage through CLI
